{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> <code>lstchain</code> DL1b to DL2 stage - Solutions </center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise section \n",
    "## train/test data set separation & RF training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Restore the state of the DL1 directory\n",
    "    - Move all the testing and trainig files to the particle's directory\n",
    "    - Erase the merged DL1 files & the DL2 files & the models\n",
    "- Split train/test data into 80/20\n",
    "    - TIP: You can use the scikit-learn to ease this process (f.ex: `from sklearn.model_selection import train_test_split`)\n",
    "- Train your dataset using the `disp_vector` option by changing this parameter in the configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, we define the absolute paths that we will be using in this notebook\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "lst_ana_repo_dir = Path('../data').resolve().absolute()\n",
    "\n",
    "mc_data_dir = Path.joinpath(lst_ana_repo_dir,'mc')\n",
    "mc_dl1_data_dir = Path.joinpath(lst_ana_repo_dir, 'mc/DL1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the original state of the `/data/mc/DL1` dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first move the data to its origin structure, so that we can perform again the train/test splitting and merging.\n",
    "\n",
    "%cd {mc_data_dir}\n",
    "!rm -rf DL2/ models/\n",
    "%cd {mc_dl1_data_dir}\n",
    "!rm gamma/dl1_gamma_training.h5 gamma/dl1_gamma_testing.h5\n",
    "!rm proton/dl1_proton_training.h5 proton/dl1_proton_testing.h5\n",
    "!rm electron/dl1_electron_training.h5 electron/dl1_electron_testing.h5\n",
    "!rm gamma-diffuse/dl1_gamma-diffuse_training.h5 gamma-diffuse/dl1_gamma-diffuse_testing.h5\n",
    "%cd gamma\n",
    "!mv testing/* . && mv training/* . && rm -rf training testing\n",
    "%cd ../proton\n",
    "!mv testing/* . && mv training/* . && rm -rf training testing\n",
    "%cd ../electron\n",
    "!mv testing/* . && mv training/* . && rm -rf training testing\n",
    "%cd ../gamma-diffuse\n",
    "!mv testing/* . && mv training/* . && rm -rf training testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the train/test dataset into a 80/20 ratio\n",
    "\n",
    "#### TIP\n",
    "You can use `from sklearn.model_selection import train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we create a list with all the files within the /DL1/gamma dir\n",
    "\n",
    "files = [file.as_posix() for file in mc_dl1_data_dir.joinpath('gamma').iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split data into a 80/20 ratio\n",
    "\n",
    "training, testing = train_test_split(files, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the spliting was done correctly\n",
    "\n",
    "len(training), len(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And move the files into the /training and /testing sub-dirs\n",
    "\n",
    "mc_dl1_data_dir.joinpath('gamma/training').mkdir(exist_ok=True)\n",
    "mc_dl1_data_dir.joinpath('gamma/testing').mkdir(exist_ok=True)\n",
    "\n",
    "for file in training:\n",
    "    shutil.move(file, mc_dl1_data_dir.joinpath('gamma/training'))\n",
    "for file in testing:\n",
    "    shutil.move(file, mc_dl1_data_dir.joinpath('gamma/testing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same for the rest of the files\n",
    "\n",
    "for particle in ['gamma-diffuse', 'proton', 'electron']:\n",
    "    \n",
    "    files = [file.as_posix() for file in mc_dl1_data_dir.joinpath(particle).iterdir()]\n",
    "    training, testing = train_test_split(files, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f'Working with {particle}. Training size: {len(training)}, testing size: {len(testing)}.')\n",
    "    \n",
    "    mc_dl1_data_dir.joinpath(particle, 'training').mkdir(exist_ok=True)\n",
    "    mc_dl1_data_dir.joinpath(particle, 'testing').mkdir(exist_ok=True)\n",
    "    \n",
    "    for file in training:\n",
    "        shutil.move(file, mc_dl1_data_dir.joinpath(particle, 'training'))\n",
    "    for file in testing:\n",
    "        shutil.move(file, mc_dl1_data_dir.joinpath(particle, 'testing'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will need to merge again the DL1 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for particle in ['gamma', 'gamma-diffuse', 'proton', 'electron']:\n",
    "    \n",
    "    source_dir = mc_dl1_data_dir.joinpath(particle, 'training').as_posix()\n",
    "    output_file = mc_dl1_data_dir.joinpath(particle, f'dl1_{particle}_training.h5').as_posix()\n",
    "    !lstchain_merge_hdf5_files -d $source_dir -o $output_file\n",
    "    \n",
    "    source_dir = mc_dl1_data_dir.joinpath(particle, 'testing').as_posix()\n",
    "    output_file = mc_dl1_data_dir.joinpath(particle, f'dl1_{particle}_testing.h5').as_posix()\n",
    "    !lstchain_merge_hdf5_files -d $source_dir -o $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check that dl1 merged files were correctly created\n",
    "\n",
    "for particle in ['gamma', 'gamma-diffuse', 'proton', 'electron']:\n",
    "    dl1_particle_dir = mc_dl1_data_dir.joinpath(particle).as_posix()\n",
    "    print(f' * {particle} directory:')\n",
    "    !ls $dl1_particle_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create a new configuration changing the RF `disp_method` \n",
    "\n",
    "Have a look to the first item of the `new_rf_config` object, as well as to the features changed in the `particle_classification_features` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from traitlets.config import Config\n",
    "\n",
    "new_rf_config = Config({\n",
    "    \n",
    "  \"disp_method\": \"disp_vector\",\n",
    "    \n",
    "  \"random_forest_energy_regressor_args\": {\n",
    "    \"max_depth\": 50,\n",
    "    \"min_samples_leaf\": 2,\n",
    "    \"n_jobs\": 4,\n",
    "    \"n_estimators\": 150,\n",
    "    \"bootstrap\": True,\n",
    "    \"criterion\": \"squared_error\",\n",
    "    \"max_features\": \"auto\",\n",
    "    \"max_leaf_nodes\": None,\n",
    "    \"min_impurity_decrease\": 0.0,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_weight_fraction_leaf\": 0.0,\n",
    "    \"oob_score\": False,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 0,\n",
    "    \"warm_start\": False\n",
    "  },\n",
    "\n",
    "  \"random_forest_disp_regressor_args\": {\n",
    "    \"max_depth\": 50,\n",
    "    \"min_samples_leaf\": 2,\n",
    "    \"n_jobs\": 4,\n",
    "    \"n_estimators\": 150,\n",
    "    \"bootstrap\": True,\n",
    "    \"criterion\": \"squared_error\",\n",
    "    \"max_features\": \"auto\",\n",
    "    \"max_leaf_nodes\": None,\n",
    "    \"min_impurity_decrease\": 0.0,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_weight_fraction_leaf\": 0.0,\n",
    "    \"oob_score\": False,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 0,\n",
    "    \"warm_start\": False\n",
    "  },\n",
    "\n",
    "  \"random_forest_disp_classifier_args\": {\n",
    "    \"max_depth\": 100,\n",
    "    \"min_samples_leaf\": 2,\n",
    "    \"n_jobs\": 4,\n",
    "    \"n_estimators\": 100,\n",
    "    \"criterion\": \"gini\",\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_weight_fraction_leaf\": 0.0,\n",
    "    \"max_features\": \"auto\",\n",
    "    \"max_leaf_nodes\": None,\n",
    "    \"min_impurity_decrease\": 0.0,\n",
    "    \"bootstrap\": True,\n",
    "    \"oob_score\": False,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 0.0,\n",
    "    \"warm_start\": False,\n",
    "    \"class_weight\": None\n",
    "  },\n",
    "\n",
    "  \"random_forest_particle_classifier_args\": {\n",
    "    \"max_depth\": 100,\n",
    "    \"min_samples_leaf\": 2,\n",
    "    \"n_jobs\": 4,\n",
    "    \"n_estimators\": 100,\n",
    "    \"criterion\": \"gini\",\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_weight_fraction_leaf\": 0.0,\n",
    "    \"max_features\": \"auto\",\n",
    "    \"max_leaf_nodes\": None,\n",
    "    \"min_impurity_decrease\": 0.0,\n",
    "    \"bootstrap\": True,\n",
    "    \"oob_score\": False,\n",
    "    \"random_state\": 42,\n",
    "    \"verbose\": 0.0,\n",
    "    \"warm_start\": False,\n",
    "    \"class_weight\": None\n",
    "  },\n",
    "\n",
    "  \"energy_regression_features\": [\n",
    "    \"log_intensity\",\n",
    "    \"width\",\n",
    "    \"length\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"wl\",\n",
    "    \"skewness\",\n",
    "    \"kurtosis\",\n",
    "    \"time_gradient\",\n",
    "    \"leakage_intensity_width_2\"\n",
    "  ],\n",
    "\n",
    "  \"disp_regression_features\": [\n",
    "    \"log_intensity\",\n",
    "    \"width\",\n",
    "    \"length\",\n",
    "    \"wl\",\n",
    "    \"skewness\",\n",
    "    \"kurtosis\",\n",
    "    \"time_gradient\",\n",
    "    \"leakage_intensity_width_2\"\n",
    "  ],\n",
    "\n",
    "  \"disp_classification_features\": [\n",
    "    \"log_intensity\",\n",
    "    \"width\",\n",
    "    \"length\",\n",
    "    \"wl\",\n",
    "    \"skewness\",\n",
    "    \"kurtosis\",\n",
    "    \"time_gradient\",\n",
    "    \"leakage_intensity_width_2\"\n",
    "  ],\n",
    "\n",
    "  \"particle_classification_features\": [\n",
    "    \"log_intensity\",\n",
    "    \"width\",\n",
    "    \"length\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"wl\",\n",
    "    \"skewness\",\n",
    "    \"kurtosis\",\n",
    "    \"time_gradient\",\n",
    "    \"leakage_intensity_width_2\",\n",
    "    \"log_reco_energy\",\n",
    "    #\"reco_disp_norm\",\n",
    "    #\"reco_disp_sign\"\n",
    "    \"reco_disp_dx\",\n",
    "    \"reco_disp_dy\"\n",
    "  ],\n",
    "\n",
    "  \"source_dependent\": False,\n",
    "  \"allowed_tels\": [1]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And we train again, this time in an alternative way, not using the `lstchain` entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstchain.reco.dl1_to_dl2 import build_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mc_dl1_data_dir.joinpath('models').mkdir(exist_ok=True)\n",
    "models_dir = mc_dl1_data_dir.joinpath('models').as_posix()\n",
    "\n",
    "dl1_gamma_diffuse_file = mc_dl1_data_dir.joinpath('gamma-diffuse/dl1_gamma-diffuse_training.h5')\n",
    "dl1_proton_file = mc_dl1_data_dir.joinpath('proton/dl1_proton_training.h5')\n",
    "\n",
    "build_models(dl1_gamma_diffuse_file,\n",
    "            dl1_proton_file,\n",
    "            save_models=True,\n",
    "            path_models=models_dir,\n",
    "            custom_config=new_rf_config\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $models_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "If you are using the school environment, or an environment with `lstchain-v0.8.4`, there is an error with the models' file name (`reg_disp_norm.sav ` and `cls_disp_sign.sav` should not be present). This bug is solved in later `lstchain` versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
