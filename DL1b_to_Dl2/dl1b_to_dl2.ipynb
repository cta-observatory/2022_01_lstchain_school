{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a2531e",
   "metadata": {},
   "source": [
    "<h1><center> <code>lstchain</code> DL1b to DL2 stage</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a743e6e4",
   "metadata": {},
   "source": [
    "## Content\n",
    "In this notebook we will go through the following topics:\n",
    " - (dataset separation into training and testing sets)\n",
    " - Merging of DL1 sub_runs\n",
    " - Training machine learning models for lstchain\n",
    " - `lstchain` DL1 to DL2 stage\n",
    " \n",
    " \n",
    "All these stages can be run locally - no need of the IT La Palma cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f4b1f",
   "metadata": {},
   "source": [
    "## 0. Training/testing dataset separation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d090a12",
   "metadata": {},
   "source": [
    "<center> <b>Note</b> that this stage is <i>usually</i> done before the R0 to DL1 stage</center>\n",
    "\n",
    "This way, the `r0_to_dl1` stage is optimised by running parallel jobs on simtel sub-runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeaf74a",
   "metadata": {},
   "source": [
    "The goal of the training/testing dataset separation is to split (randomly) the files of a dataset into these two sets. So that they can be easily used in later stages.\n",
    " - Classical machine learning algorithms set (generally) the train/test ratio to 80/20.\n",
    "\n",
    "For this lecture, we will just make a \"rough\" 50/50 train/test separation (of DL1 data). \\\n",
    "Note that current MC productions are composed between 1000 to 5000 files (dependant of the particle).\n",
    "\n",
    "An exercise (end of this notebook) is proposed to do the train/test splitting in a more formal way, using lstchain tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf440a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define all the paths needed to run this notebook\n",
    "from pathlib import Path\n",
    "\n",
    "lst_ana_repo_dir = Path('../data').resolve().absolute()\n",
    "\n",
    "mc_data_dir = Path.joinpath(lst_ana_repo_dir,'mc')\n",
    "mc_dl1_data_dir = Path.joinpath(lst_ana_repo_dir, 'mc/DL1')\n",
    "\n",
    "%cd {mc_dl1_data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ffc04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check how data is stored\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls gamma | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af01530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with gammas, later we will do similar with the rest of the particles\n",
    "%cd gamma\n",
    "!mkdir -p testing training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f36960",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e02f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "mv `ls *.h5 | head -n 10` testing && mv *.h5 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12469b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check data was moved correctly \n",
    "!ls testing | wc -l && ls training | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the rest of the files\n",
    "%cd ..\n",
    "!mkdir -p electron/testing electron/training\n",
    "!mkdir -p proton/testing proton/training\n",
    "!mkdir -p gamma-diffuse/testing gamma-diffuse/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd electron\n",
    "mv `ls *.h5 | head -n 10` testing && mv *.h5 training\n",
    "\n",
    "cd ../proton\n",
    "mv `ls *.h5 | head -n 10` testing && mv *.h5 training\n",
    "\n",
    "cd ../gamma-diffuse\n",
    "mv `ls *.h5 | head -n 10` testing && mv *.h5 training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ef438d",
   "metadata": {},
   "source": [
    "## 1. Merging of lstchain MC DL1 sub_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca53df0",
   "metadata": {},
   "source": [
    "Once all the sub-runs have been converted into the DL1 stage, it is advised to merge all the files into a single one to ease the rest of the analysis.\n",
    "\n",
    "To do so, we will just use the `lstchain_merge_hdf5_file` entry point*.\n",
    "\n",
    "$^{\\ast}$An entry point is a program that comes together with the installation of lstchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lstchain_merge_hdf5_files -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea403d5a",
   "metadata": {},
   "source": [
    "We move to the directory where the MC DL1 data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0200a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {mc_dl1_data_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf98f5e",
   "metadata": {},
   "source": [
    "and start merging the data, separated by particle classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d71cdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example with gamma\n",
    "!lstchain_merge_hdf5_files -d gamma/training -o gamma/dl1_gamma_training.h5\n",
    "!lstchain_merge_hdf5_files -d gamma/testing -o gamma/dl1_gamma_testing.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e739cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f645b2d0",
   "metadata": {},
   "source": [
    "and finally for the rest of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72e83e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!lstchain_merge_hdf5_files -d gamma-diffuse/training -o gamma-diffuse/dl1_gamma-diffuse_training.h5\n",
    "!lstchain_merge_hdf5_files -d gamma-diffuse/testing -o gamma-diffuse/dl1_gamma-diffuse_testing.h5\n",
    "\n",
    "!lstchain_merge_hdf5_files -d electron/training -o electron/dl1_electron_training.h5\n",
    "!lstchain_merge_hdf5_files -d electron/testing -o electron/dl1_electron_testing.h5\n",
    "\n",
    "!lstchain_merge_hdf5_files -d proton/training -o proton/dl1_proton_training.h5\n",
    "!lstchain_merge_hdf5_files -d proton/testing -o proton/dl1_proton_testing.h5\n",
    "\n",
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39adb301",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l proton\n",
    "!ls -l electron\n",
    "!ls -l gamma-diffuse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffbc156",
   "metadata": {},
   "source": [
    "## 2. Training of Random Forest models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a41aed",
   "metadata": {},
   "source": [
    "`lstchain` uses 'classical' machine learning (ML) algorithms that are applied during the `dl1_to_dl2` stage to perform:\n",
    " - gamma/hadron separation,\n",
    " - energy reconstruction,\n",
    " - direction reconstruction.\n",
    " \n",
    "These ML algorithms, that are trained with DL1 data, are\n",
    " - Random Forest regressor (energy and direction reconstruction)\n",
    " - Random Forest classifier (gamma/hadron separation)\n",
    "loaded from the `scikit-learn` python pibrary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb6406",
   "metadata": {},
   "source": [
    "The set of parameters used to train the models are defined in a configuration file.\n",
    "\n",
    "For not expert users, standard parameters (those found in the lstchain_standard_config.json file) should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ad89a",
   "metadata": {},
   "source": [
    "Currently, the user can select between two kinds of direction reconstruction trainings:\n",
    " - `disp_ver`\n",
    "     * A single RF regressor is trained for the `disp_norm` vector coordinates.\n",
    " - `disp_norm_sign`   (**Default choice**)\n",
    "     * A RF regressor is trained for the module of the `disp_norm` vector.\n",
    "     * A RF classifier to classify the `disp_norm` sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7387740",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {mc_data_dir}\n",
    "!cat configs/lstchain_trainpipe_dl1b_dl2_config.json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28392aa4",
   "metadata": {},
   "source": [
    "### `lstchain` RF traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3713e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train our models from the previously merged DL1 files\n",
    "!mkdir -p models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2fa74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lstchain_mc_trainpipe -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185c6c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lstchain_mc_trainpipe --fg DL1/gamma-diffuse/dl1_gamma-diffuse_training.h5 \\\n",
    " --fp DL1/proton/dl1_proton_training.h5 -o models -c configs/lstchain_trainpipe_dl1b_dl2_config.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a859757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd19d4",
   "metadata": {},
   "source": [
    "### We can also plot the RF parameter importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a185ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from lstchain.visualization.plot_dl2 import plot_models_features_importances\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a0cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_models_features_importances(path_models='models', config_file='configs/lstchain_trainpipe_dl1b_dl2_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3fbb50",
   "metadata": {},
   "source": [
    "## 3. `lstchain` DL1 to DL2 stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87d7ef",
   "metadata": {},
   "source": [
    "In this stage, the trained models are applied to DL1 data.\n",
    "\n",
    "Energy and position are reconstructed, and gamma/hadron separation applied (probability of an event to be a gamma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the path tree\n",
    "%cd {mc_data_dir}\n",
    "!mkdir -p DL2\n",
    "%cd DL2\n",
    "!mkdir -p proton electron gamma gamma-diffuse\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc7869",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lstchain_dl1_to_dl2 -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with the gamma file\n",
    "!lstchain_dl1_to_dl2 -f DL1/gamma/dl1_gamma_testing.h5 -p models/ -o DL2/gamma -c configs/lstchain_trainpipe_dl1b_dl2_config.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d617f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And for all the rest of particles\n",
    "!lstchain_dl1_to_dl2 -f DL1/proton/dl1_proton_testing.h5 -p models/ -o DL2/proton -c configs/lstchain_trainpipe_dl1b_dl2_config.json \n",
    "!lstchain_dl1_to_dl2 -f DL1/electron/dl1_electron_testing.h5 -p models/ -o DL2/electron -c configs/lstchain_trainpipe_dl1b_dl2_config.json \n",
    "!lstchain_dl1_to_dl2 -f DL1/gamma-diffuse/dl1_gamma-diffuse_testing.h5 -p models/ -o DL2/gamma-diffuse -c configs/lstchain_trainpipe_dl1b_dl2_config.json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa67cb7",
   "metadata": {},
   "source": [
    "#### Let's check which parameters have been added in the `dl1_to_dl2` stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920f59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "hf_dl1 = tables.open_file('DL1/gamma/dl1_gamma_testing.h5')\n",
    "hf_dl2 = tables.open_file('DL2/gamma/dl2_gamma_testing.h5')\n",
    "\n",
    "from lstchain.io.io import dl1_params_lstcam_key, dl2_params_lstcam_key\n",
    "dl1_parameters = hf_dl1.root[dl1_params_lstcam_key].colnames\n",
    "dl2_parameters = hf_dl2.root[dl2_params_lstcam_key].colnames\n",
    "\n",
    "set(dl2_parameters)-set(dl1_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd437d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the existing DL1 parameters\n",
    "dl1_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f1cb7",
   "metadata": {},
   "source": [
    "### Tip\n",
    "To explore any HDF5 file through a GUI, you can use https://vitables.org/ (among many other ways)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731a34d",
   "metadata": {},
   "source": [
    "<h1><center> Exercise section </center></h1>\n",
    "<h2> train/test data set separation & RF training </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f24995",
   "metadata": {},
   "source": [
    "- Restore the state of the DL1 directory\n",
    "    - Move all the testing and trainig files to the particle's directory\n",
    "    - Erase the merged DL1 files & the DL2 files & the models\n",
    "- Split train/test data into 80/20\n",
    "    - TIP: You can use the scikit-learn to ease this process (f.ex: `from sklearn.model_selection import train_test_split`)\n",
    "- Train your dataset using the `disp_ver` option by changing this parameter in the configuration file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0cca5",
   "metadata": {},
   "source": [
    "## RESTORE THE ORIGINAL STATE OF THE DL1 DIRECTORY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c93e91",
   "metadata": {},
   "source": [
    "Convert the below cell into `code` (cell --> cell type --> code) and run it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ff2de",
   "metadata": {},
   "source": [
    "%cd {mc_data_dir}\n",
    "!rm -rf DL2/ models/\n",
    "%cd {mc_dl1_data_dir}\n",
    "!rm gamma/dl1_gamma_training.h5 gamma/dl1_gamma_testing.h5\n",
    "!rm proton/dl1_proton_training.h5 proton/dl1_proton_testing.h5\n",
    "!rm electron/dl1_electron_training.h5 electron/dl1_electron_testing.h5\n",
    "!rm gamma-diffuse/dl1_gamma-diffuse_training.h5 gamma-diffuse/dl1_gamma-diffuse_testing.h5\n",
    "%cd gamma\n",
    "!mv testing/* . && mv training/* . && rm -rf training testing\n",
    "%cd ../proton\n",
    "!mv testing/* . && mv training/* . && rm -rf training testing\n",
    "%cd ../electron\n",
    "!mv testing/* . && mv training/* . && rm -rf training testing\n",
    "%cd ../gamma-diffuse\n",
    "!mv testing/* . && mv training/* . && rm -rf training testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5397733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
